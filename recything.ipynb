{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully created\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Konfigurasi Faker\n",
    "fake = Faker('id_ID')\n",
    "\n",
    "# Jumlah data yang akan di-generate\n",
    "num_records = 150\n",
    "\n",
    "# Function untuk generate data berdasarkan skema tabel\n",
    "def generate_data(num_records, table_name):\n",
    "    data = []\n",
    "    if table_name == 'reports':\n",
    "        report_types = ['rubbish', 'littering']\n",
    "        waste_types = {\n",
    "            'rubbish': ['sampah basah', 'sampah kering', 'sampah basah, sampah kering'],\n",
    "            'littering': ['organik', 'anorganik', 'berbahaya']\n",
    "        }\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            report_type = random.choice(report_types)\n",
    "            waste_type = random.choice(waste_types[report_type])\n",
    "            \n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'report_type': report_type,\n",
    "                'waste_type': waste_type,\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'status': random.choice(status_types),\n",
    "                'reason': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'location_reports':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'longtitude': fake.longitude(),\n",
    "                'langtitude': fake.latitude(),\n",
    "                'address': fake.address(),\n",
    "                'city': fake.city(),\n",
    "                'province': fake.state()\n",
    "            })\n",
    "    elif table_name == 'users':\n",
    "        genders = ['laki-laki', 'perempuan']\n",
    "        badges = ['bronze', 'silver', 'gold', 'platinum']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'email': fake.email(),\n",
    "                'phone_number': fake.random_number(digits=16, fix_len=True),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'gender': random.choice(genders),\n",
    "                'birth_date': fake.date_of_birth(),\n",
    "                'address': fake.address(),\n",
    "                'badge': random.choice(badges),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'waste_materials':\n",
    "        material_types = ['plastik', 'kaca', 'kayu', 'kertas', 'baterai', 'besi', 'limbah berbahaya', 'limbah beracun', 'sisa makanan', 'tak terdeteksi']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'type': random.choice(material_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'task_users_challange':\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'status': random.choice(status_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'start_date': fake.date_time(),\n",
    "                'end_date': fake.date_time(),\n",
    "                'status': fake.boolean(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_steps':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'admins':\n",
    "        roles = ['admin', 'super_admin']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'role': random.choice(roles),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'contents':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title_content': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'categories':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.word(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Function untuk generate data untuk tabel fakta\n",
    "def generate_fact_data(num_records, fact_table_name, dimensions):\n",
    "    data = []\n",
    "    if fact_table_name == 'fact_reporting':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'reports_id': random.choice(dimensions['reports']['id'].values),\n",
    "                'waste_materials_id': random.choice(dimensions['waste_materials']['id'].values),\n",
    "                'location_reports_id': random.choice(dimensions['location_reports']['id'].values),\n",
    "                'count_reporting': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'task_users_challange_id': random.choice(dimensions['task_users_challange']['id'].values),\n",
    "                'task_steps_id': random.choice(dimensions['task_steps']['id'].values),\n",
    "                'task_challange_id': random.choice(dimensions['task_challange']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_challange': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_conten':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'contents_id': random.choice(dimensions['contents']['id'].values),\n",
    "                'categories_id': random.choice(dimensions['categories']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_conten': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk setiap tabel dimensi\n",
    "tables = [\n",
    "    'reports', 'location_reports', 'users', 'waste_materials', 'task_users_challange',\n",
    "    'task_challange', 'task_steps', 'admins', 'contents', 'categories'\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    df = pd.DataFrame(generate_data(num_records, table))\n",
    "    df.to_csv(f'data_dummy/{table}.csv', index=False)\n",
    "\n",
    "# Baca data dimensi dari file CSV yang sudah dibuat sebelumnya\n",
    "dimensions = {}\n",
    "for table in tables:\n",
    "    dimensions[table] = pd.read_csv(f'data_dummy/{table}.csv')\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk tabel fakta\n",
    "fact_tables = ['fact_reporting', 'fact_challange', 'fact_conten']\n",
    "for fact_table in fact_tables:\n",
    "    df = pd.DataFrame(generate_fact_data(num_records, fact_table, dimensions))\n",
    "    df.to_csv(f'data_dummy/{fact_table}.csv', index=False)\n",
    "print(\"Data successfully created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from database back end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yazid\\AppData\\Local\\Temp\\ipykernel_20780\\2175763943.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\yazid\\AppData\\Local\\Temp\\ipykernel_20780\\2175763943.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\yazid\\AppData\\Local\\Temp\\ipykernel_20780\\2175763943.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\yazid\\AppData\\Local\\Temp\\ipykernel_20780\\2175763943.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Ekstraksi semua tabel ke file CSV\n",
    "def extract_all_tables_to_csv(output_folder):\n",
    "    try:\n",
    "        with mysql.connector.connect(\n",
    "            host=os.getenv(\"host\"),\n",
    "            port=\"3306\",\n",
    "            user=\"root\",\n",
    "            password=os.getenv(\"password\"),\n",
    "            database=\"recything_db\"\n",
    "        ) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SHOW TABLES\")\n",
    "            tables = cursor.fetchall()\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                query = f\"SELECT * FROM {table_name}\"\n",
    "                df = pd.read_sql(query, conn)\n",
    "                file_path = f\"{output_folder}/{table_name}.csv\"\n",
    "                df.to_csv(file_path, index=False)\n",
    "            print(\"Data extraction completed successfully\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "# Eksekusi fungsi ekstraksi\n",
    "extract_all_tables_to_csv(\"data_database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Big Query"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded admins.csv into recything-project.data_recything.admins\n",
      "Loaded categories.csv into recything-project.data_recything.categories\n",
      "Loaded contents.csv into recything-project.data_recything.contents\n",
      "Loaded fact_challange.csv into recything-project.data_recything.fact_challange\n",
      "Loaded fact_conten.csv into recything-project.data_recything.fact_conten\n",
      "Loaded fact_reporting.csv into recything-project.data_recything.fact_reporting\n",
      "Loaded location_reports.csv into recything-project.data_recything.location_reports\n",
      "Loaded reports.csv into recything-project.data_recything.reports\n",
      "Loaded task_challange.csv into recything-project.data_recything.task_challange\n",
      "Loaded task_steps.csv into recything-project.data_recything.task_steps\n",
      "Loaded task_users_challange.csv into recything-project.data_recything.task_users_challange\n",
      "Loaded users.csv into recything-project.data_recything.users\n",
      "Loaded waste_materials.csv into recything-project.data_recything.waste_materials\n",
      "Data successfully uploaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Path to the JSON key file\n",
    "key_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Ensure key_path is correctly loaded\n",
    "if not key_path:\n",
    "    raise ValueError(\"GOOGLE_APPLICATION_CREDENTIALS key path not found. Please check your .env file and environment variables.\")\n",
    "\n",
    "# Set the Google Cloud credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n",
    "\n",
    "# BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define the existing dataset name\n",
    "dataset_name = 'data_recything'\n",
    "\n",
    "# Folder containing the CSV files\n",
    "data_folder = 'data_dummy'\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'admins.csv', 'categories.csv', 'contents.csv', 'fact_challange.csv',\n",
    "    'fact_conten.csv', 'fact_reporting.csv', 'location_reports.csv', 'reports.csv',\n",
    "    'task_challange.csv', 'task_steps.csv', 'task_users_challange.csv', 'users.csv',\n",
    "    'waste_materials.csv'\n",
    "]\n",
    "\n",
    "# Function to load CSV file into BigQuery\n",
    "def load_csv_to_bigquery(file_name):\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    table_name = file_name.replace('.csv', '')\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define the table ID\n",
    "    table_id = f\"{client.project}.{dataset_name}.{table_name}\"\n",
    "    \n",
    "    # Load DataFrame to BigQuery table\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    \n",
    "    print(f\"Loaded {file_name} into {table_id}\")\n",
    "\n",
    "# Load each CSV file into BigQuery\n",
    "for csv_file in csv_files:\n",
    "    load_csv_to_bigquery(csv_file)\n",
    "print(\"Data successfully uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
