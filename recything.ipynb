{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Konfigurasi Faker\n",
    "fake = Faker('id_ID')\n",
    "\n",
    "# Jumlah data yang akan di-generate\n",
    "num_records = 150\n",
    "\n",
    "# Function untuk generate data berdasarkan skema tabel\n",
    "def generate_data(num_records, table_name):\n",
    "    data = []\n",
    "    if table_name == 'reports':\n",
    "        report_types = ['rubbish', 'littering']\n",
    "        waste_types = ['sampah basah', 'sampah kering', 'sampah basah,sampah kering', 'organik', 'anorganik', 'berbahaya']\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'report_type': random.choice(report_types),\n",
    "                'waste_type': random.choice(waste_types),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'status': random.choice(status_types),\n",
    "                'reason': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'location_reports':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'longtitude': fake.longitude(),\n",
    "                'langtitude': fake.latitude(),\n",
    "                'address': fake.address(),\n",
    "                'city': fake.city(),\n",
    "                'province': fake.state()\n",
    "            })\n",
    "    elif table_name == 'users':\n",
    "        genders = ['laki-laki', 'perempuan']\n",
    "        badges = ['bronze', 'silver', 'gold', 'platinum']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'email': fake.email(),\n",
    "                'phone_number': fake.random_number(digits=16, fix_len=True),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'gender': random.choice(genders),\n",
    "                'birth_date': fake.date_of_birth(),\n",
    "                'address': fake.address(),\n",
    "                'badge': random.choice(badges),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'waste_materials':\n",
    "        material_types = ['plastik', 'kaca', 'kayu', 'kertas', 'baterai', 'besi', 'limbah berbahaya', 'limbah beracun', 'sisa makanan', 'tak terdeteksi']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'type': random.choice(material_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'task_users_challange':\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'status': random.choice(status_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'start_date': fake.date_time(),\n",
    "                'end_date': fake.date_time(),\n",
    "                'status': fake.boolean(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_steps':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'admins':\n",
    "        roles = ['admin', 'super_admin']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'role': random.choice(roles),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'contents':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title_content': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'categories':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.word(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Function untuk generate data untuk tabel fakta\n",
    "def generate_fact_data(num_records, fact_table_name, dimensions):\n",
    "    data = []\n",
    "    if fact_table_name == 'fact_reporting':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'reports_id': random.choice(dimensions['reports']['id'].values),\n",
    "                'waste_materials_id': random.choice(dimensions['waste_materials']['id'].values),\n",
    "                'location_reports_id': random.choice(dimensions['location_reports']['id'].values),\n",
    "                'count_reporting': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'task_users_challange_id': random.choice(dimensions['task_users_challange']['id'].values),\n",
    "                'task_steps_id': random.choice(dimensions['task_steps']['id'].values),\n",
    "                'task_challange_id': random.choice(dimensions['task_challange']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_challange': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_conten':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'contents_id': random.choice(dimensions['contents']['id'].values),\n",
    "                'categories_id': random.choice(dimensions['categories']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_conten': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk setiap tabel dimensi\n",
    "tables = [\n",
    "    'reports', 'location_reports', 'users', 'waste_materials', 'task_users_challange',\n",
    "    'task_challange', 'task_steps', 'admins', 'contents', 'categories'\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    df = pd.DataFrame(generate_data(num_records, table))\n",
    "    df.to_csv(f'data_source/{table}.csv', index=False)\n",
    "\n",
    "# Baca data dimensi dari file CSV yang sudah dibuat sebelumnya\n",
    "dimensions = {}\n",
    "for table in tables:\n",
    "    dimensions[table] = pd.read_csv(f'data_source/{table}.csv')\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk tabel fakta\n",
    "fact_tables = ['fact_reporting', 'fact_challange', 'fact_conten']\n",
    "for fact_table in fact_tables:\n",
    "    df = pd.DataFrame(generate_fact_data(num_records, fact_table, dimensions))\n",
    "    df.to_csv(f'data_source/{fact_table}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(bucket_name, files):\n",
    "    # Inisialisasi client GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Upload setiap file dari folder data_source ke bucket GCS\n",
    "    for file in files:\n",
    "        source_file = os.path.join(\"data_source\", file)\n",
    "        blob = bucket.blob(file)\n",
    "        blob.upload_from_filename(source_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti dengan nama bucket Anda di GCS\n",
    "    bucket_name = \"nama_bucket_anda\"\n",
    "\n",
    "    # Daftar file CSV yang akan diunggah dari folder data_source\n",
    "    files = [\n",
    "        \"admins.csv\",\n",
    "        \"categories.csv\",\n",
    "        \"contents.csv\",\n",
    "        \"fact_challange.csv\",\n",
    "        \"fact_content.csv\",\n",
    "        \"fact_reporting.csv\",\n",
    "        \"location_reports.csv\",\n",
    "        \"reports.csv\",\n",
    "        \"task_challange.csv\",\n",
    "        \"task_steps.csv\",\n",
    "        \"task_users_challange.csv\",\n",
    "        \"users.csv\",\n",
    "        \"waste_materials.csv\"\n",
    "    ]\n",
    "\n",
    "    # Panggil fungsi untuk mengunggah file ke bucket GCS\n",
    "    upload_to_gcs(bucket_name, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def load_csv_to_bigquery(dataset_id, table_id, source_file):\n",
    "    # Inisialisasi client BigQuery\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Buat dataset jika belum ada\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    try:\n",
    "        dataset = client.create_dataset(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Dataset {dataset_id} already exists\")\n",
    "\n",
    "    # Load data dari file CSV ke dalam tabel BigQuery\n",
    "    table_ref = dataset.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.skip_leading_rows = 1  # Skip header row\n",
    "    job_config.autodetect = True  # Auto detect schema\n",
    "\n",
    "    with open(source_file, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "    job.result()  # Tunggu hingga job selesai\n",
    "\n",
    "    print(f\"Data from {source_file} loaded into {dataset_id}.{table_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti dengan informasi proyek dan dataset BigQuery Anda\n",
    "    project_id = \"nama_project_anda\"\n",
    "    dataset_id = \"nama_dataset_anda\"\n",
    "    \n",
    "    # Tentukan folder sumber data\n",
    "    source_folder = \"data_source\"\n",
    "\n",
    "    # Daftar file CSV yang akan dimuat ke BigQuery\n",
    "    files = [\n",
    "        {\"file\": \"admins.csv\", \"table\": \"admins\"},\n",
    "        {\"file\": \"categories.csv\", \"table\": \"categories\"},\n",
    "        {\"file\": \"contents.csv\", \"table\": \"contents\"},\n",
    "        {\"file\": \"fact_challange.csv\", \"table\": \"fact_challange\"},\n",
    "        {\"file\": \"fact_content.csv\", \"table\": \"fact_content\"},\n",
    "        {\"file\": \"fact_reporting.csv\", \"table\": \"fact_reporting\"},\n",
    "        {\"file\": \"location_reports.csv\", \"table\": \"location_reports\"},\n",
    "        {\"file\": \"reports.csv\", \"table\": \"reports\"},\n",
    "        {\"file\": \"task_challange.csv\", \"table\": \"task_challange\"},\n",
    "        {\"file\": \"task_steps.csv\", \"table\": \"task_steps\"},\n",
    "        {\"file\": \"task_users_challange.csv\", \"table\": \"task_users_challange\"},\n",
    "        {\"file\": \"users.csv\", \"table\": \"users\"},\n",
    "        {\"file\": \"waste_materials.csv\", \"table\": \"waste_materials\"}\n",
    "    ]\n",
    "\n",
    "    # Muat setiap file CSV ke dalam tabel BigQuery\n",
    "    for file in files:\n",
    "        source_file = os.path.join(source_folder, file[\"file\"])\n",
    "        table_id = file[\"table\"]\n",
    "        load_csv_to_bigquery(dataset_id, table_id, source_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
