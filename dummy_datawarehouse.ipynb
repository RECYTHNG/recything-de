{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Konfigurasi Faker\n",
    "fake = Faker('id_ID')\n",
    "\n",
    "# Jumlah data yang akan di-generate\n",
    "num_records = 150\n",
    "\n",
    "# Function untuk generate data berdasarkan skema tabel\n",
    "def generate_data(num_records, table_name):\n",
    "    data = []\n",
    "    if table_name == 'reports':\n",
    "        report_types = ['rubbish', 'littering']\n",
    "        waste_types = {\n",
    "            'rubbish': ['sampah basah', 'sampah kering', 'sampah basah, sampah kering'],\n",
    "            'littering': ['organik', 'anorganik', 'berbahaya']\n",
    "        }\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            report_type = random.choice(report_types)\n",
    "            waste_type = random.choice(waste_types[report_type])\n",
    "            \n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'report_type': report_type,\n",
    "                'waste_type': waste_type,\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'status': random.choice(status_types),\n",
    "                'reason': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'location_reports':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'longtitude': fake.longitude(),\n",
    "                'langtitude': fake.latitude(),\n",
    "                'address': fake.address(),\n",
    "                'city': fake.city(),\n",
    "                'province': fake.state()\n",
    "            })\n",
    "    elif table_name == 'users':\n",
    "        genders = ['laki-laki', 'perempuan']\n",
    "        badges = ['bronze', 'silver', 'gold', 'platinum']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'email': fake.email(),\n",
    "                'phone_number': fake.random_number(digits=16, fix_len=True),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'gender': random.choice(genders),\n",
    "                'birth_date': fake.date_of_birth(),\n",
    "                'address': fake.address(),\n",
    "                'badge': random.choice(badges),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'waste_materials':\n",
    "        material_types = ['plastik', 'kaca', 'kayu', 'kertas', 'baterai', 'besi', 'limbah berbahaya', 'limbah beracun', 'sisa makanan', 'tak terdeteksi']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.uuid4(),\n",
    "                'type': random.choice(material_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'task_users_challange':\n",
    "        status_types = ['need review', 'approve', 'reject']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'status': random.choice(status_types),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'point': fake.random_int(min=0, max=1000),\n",
    "                'start_date': fake.date_time(),\n",
    "                'end_date': fake.date_time(),\n",
    "                'status': fake.boolean(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'task_steps':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(max_nb_chars=255),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'admins':\n",
    "        roles = ['admin', 'super_admin']\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.name(),\n",
    "                'role': random.choice(roles),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time(),\n",
    "                'deleted_at': fake.date_time_this_year(before_now=True, after_now=False)\n",
    "            })\n",
    "    elif table_name == 'contents':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'title_content': fake.sentence(nb_words=6),\n",
    "                'description': fake.text(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    elif table_name == 'categories':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'name': fake.word(),\n",
    "                'created_at': fake.date_time(),\n",
    "                'updated_at': fake.date_time()\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Function untuk generate data untuk tabel fakta\n",
    "def generate_fact_data(num_records, fact_table_name, dimensions):\n",
    "    data = []\n",
    "    if fact_table_name == 'fact_reporting':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'reports_id': random.choice(dimensions['reports']['id'].values),\n",
    "                'waste_materials_id': random.choice(dimensions['waste_materials']['id'].values),\n",
    "                'location_reports_id': random.choice(dimensions['location_reports']['id'].values),\n",
    "                'count_reporting': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_challange':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'users_id': random.choice(dimensions['users']['id'].values),\n",
    "                'task_users_challange_id': random.choice(dimensions['task_users_challange']['id'].values),\n",
    "                'task_steps_id': random.choice(dimensions['task_steps']['id'].values),\n",
    "                'task_challange_id': random.choice(dimensions['task_challange']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_challange': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    elif fact_table_name == 'fact_conten':\n",
    "        for _ in range(num_records):\n",
    "            data.append({\n",
    "                'id': fake.random_int(min=1, max=10000),\n",
    "                'contents_id': random.choice(dimensions['contents']['id'].values),\n",
    "                'categories_id': random.choice(dimensions['categories']['id'].values),\n",
    "                'admins_id': random.choice(dimensions['admins']['id'].values),\n",
    "                'count_conten': fake.random_int(min=1, max=100)\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk setiap tabel dimensi\n",
    "tables = [\n",
    "    'reports', 'location_reports', 'users', 'waste_materials', 'task_users_challange',\n",
    "    'task_challange', 'task_steps', 'admins', 'contents', 'categories'\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    df = pd.DataFrame(generate_data(num_records, table))\n",
    "    df.to_csv(f'data_dummy/{table}.csv', index=False)\n",
    "\n",
    "# Baca data dimensi dari file CSV yang sudah dibuat sebelumnya\n",
    "dimensions = {}\n",
    "for table in tables:\n",
    "    dimensions[table] = pd.read_csv(f'data_dummy/{table}.csv')\n",
    "\n",
    "# Generate dan simpan data ke CSV untuk tabel fakta\n",
    "fact_tables = ['fact_reporting', 'fact_challange', 'fact_conten']\n",
    "for fact_table in fact_tables:\n",
    "    df = pd.DataFrame(generate_fact_data(num_records, fact_table, dimensions))\n",
    "    df.to_csv(f'data_dummy/{fact_table}.csv', index=False)\n",
    "print(\"Data successfully created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Path to the JSON key file\n",
    "key_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Ensure key_path is correctly loaded\n",
    "if not key_path:\n",
    "    raise ValueError(\"GOOGLE_APPLICATION_CREDENTIALS key path not found. Please check your .env file and environment variables.\")\n",
    "\n",
    "# Set the Google Cloud credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n",
    "\n",
    "# BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define the existing dataset name\n",
    "dataset_name = 'data_recything'\n",
    "\n",
    "# Folder containing the CSV files\n",
    "data_folder = 'data_dummy'\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'admins.csv', 'categories.csv', 'contents.csv', 'fact_challange.csv',\n",
    "    'fact_conten.csv', 'fact_reporting.csv', 'location_reports.csv', 'reports.csv',\n",
    "    'task_challange.csv', 'task_steps.csv', 'task_users_challange.csv', 'users.csv',\n",
    "    'waste_materials.csv'\n",
    "]\n",
    "\n",
    "# Function to load CSV file into BigQuery\n",
    "def load_csv_to_bigquery(file_name):\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    table_name = file_name.replace('.csv', '')\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define the table ID\n",
    "    table_id = f\"{client.project}.{dataset_name}.{table_name}\"\n",
    "    \n",
    "    # Load DataFrame to BigQuery table\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    \n",
    "    print(f\"Loaded {file_name} into {table_id}\")\n",
    "\n",
    "# Load each CSV file into BigQuery\n",
    "for csv_file in csv_files:\n",
    "    load_csv_to_bigquery(csv_file)\n",
    "print(\"Data successfully uploaded\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
